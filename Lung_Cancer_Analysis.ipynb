{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdsiamahmed26/Lung_Cancer_Analysis/blob/main/Lung_Cancer_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8T0azoHe3AR"
      },
      "source": [
        "#  **Lung Cancer Analysis **\n",
        "\n",
        "## Complete Rewrite with 13 GEO Datasets\n",
        "\n",
        "** IMPORTANT**: This version has **automatic kernel restart** to fix numpy/scipy compatibility!\n",
        "\n",
        "Just run all cells in order - the environment will fix itself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KunLk_Vje3AU"
      },
      "source": [
        "##  STEP 0: RESTART KERNEL & FIX DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7SwZ_0Re3AU",
        "outputId": "0fe2cc8c-d6ba-48e5-8a35-b41806fd1aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 0: FIXING ENVIRONMENT (This is CRITICAL!)\n",
            "================================================================================\n",
            "\n",
            "‚úì Google Colab detected\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Installing compatible packages...\n",
            "--------------------------------------------------------------------------------\n",
            "Installing numpy... ‚úì\n",
            "Installing scipy... ‚úì\n",
            "Installing pandas... ‚úì\n",
            "Installing scikit-learn... ‚úì\n",
            "Installing matplotlib... ‚úì\n",
            "Installing seaborn... ‚úì\n",
            "Installing statsmodels... ‚úì\n",
            "Installing networkx... ‚úì\n",
            "Installing GEOparse... ‚úì\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "All packages installed!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT FOR COLAB USERS:\n",
            "You MUST restart the kernel now!\n",
            "\n",
            "Go to: Runtime ‚Üí Restart Session\n",
            "\n",
            "Then run the next cell (Step 1: Import Libraries)\n",
            "\n",
            "DO NOT RUN ANY CELLS UNTIL YOU RESTART!\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Packages installed. Now RESTART YOUR KERNEL!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 0: FIXING ENVIRONMENT (This is CRITICAL!)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For Google Colab\n",
        "try:\n",
        "    from google.colab import output\n",
        "    print(\"\\n‚úì Google Colab detected\")\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"\\n‚úì Local Jupyter detected\")\n",
        "    IN_COLAB = False\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Installing compatible packages...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Install in correct order with specific versions\n",
        "packages_to_install = [\n",
        "    'numpy>=1.24.0,<2.0.0',\n",
        "    'scipy>=1.11.0',\n",
        "    'pandas>=2.0.0',\n",
        "    'scikit-learn>=1.3.0',\n",
        "    'matplotlib>=3.7.0',\n",
        "    'seaborn>=0.12.0',\n",
        "    'statsmodels>=0.14.0',\n",
        "    'networkx>=3.1',\n",
        "    'GEOparse>=2.0.0',\n",
        "]\n",
        "\n",
        "for pkg in packages_to_install:\n",
        "    print(f\"Installing {pkg.split('>=')[0]}...\", end=\" \")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\", \"--no-cache-dir\"])\n",
        "    print(\"‚úì\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"All packages installed!\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"\\n‚ö†Ô∏è  IMPORTANT FOR COLAB USERS:\")\n",
        "    print(\"You MUST restart the kernel now!\")\n",
        "    print(\"\\nGo to: Runtime ‚Üí Restart Session\")\n",
        "    print(\"\\nThen run the next cell (Step 1: Import Libraries)\")\n",
        "    print(\"\\nDO NOT RUN ANY CELLS UNTIL YOU RESTART!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  IMPORTANT FOR LOCAL JUPYTER USERS:\")\n",
        "    print(\"You MUST restart the kernel now!\")\n",
        "    print(\"\\nGo to: Kernel ‚Üí Restart Kernel\")\n",
        "    print(\"\\nThen run the next cell (Step 1: Import Libraries)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Packages installed. Now RESTART YOUR KERNEL!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maS7ldEme3AV"
      },
      "source": [
        "## ‚ö†Ô∏è DID YOU RESTART THE KERNEL?\n",
        "\n",
        "**IF NOT, DO IT NOW:**\n",
        "\n",
        "- **Google Colab**: Runtime ‚Üí Restart Session\n",
        "- **Jupyter Lab**: Kernel ‚Üí Restart Kernel\n",
        "- **Jupyter Notebook**: Kernel ‚Üí Restart\n",
        "\n",
        "**THEN come back and run the next cell!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojr2DuyJe3AV"
      },
      "source": [
        "##  STEP 1: IMPORT ALL LIBRARIES (After Restart!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMmXQM6Te3AV",
        "outputId": "9c6748d6-7f21-4a71-96e8-2c2d88b663d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 1: IMPORTING LIBRARIES (After Kernel Restart)\n",
            "================================================================================\n",
            "\n",
            "‚úÖ SUCCESS! All libraries imported!\n",
            "\n",
            "Versions:\n",
            "  ‚Ä¢ NumPy: 1.26.4\n",
            "  ‚Ä¢ Pandas: 2.2.2\n",
            "  ‚Ä¢ Scikit-learn: imported\n",
            "  ‚Ä¢ Scipy: imported\n",
            "  ‚Ä¢ GEOparse: imported\n",
            "\n",
            "‚úì Directories created: results/, data/\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ENVIRONMENT READY! Proceed to Step 2\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: IMPORTING LIBRARIES (After Kernel Restart)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    import GEOparse\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from scipy import stats\n",
        "    from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "    from scipy.spatial.distance import squareform\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "    from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    import warnings\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    # Setup\n",
        "    sns.set_style('whitegrid')\n",
        "    plt.rcParams['figure.figsize'] = (14, 9)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "\n",
        "    print(\"\\n‚úÖ SUCCESS! All libraries imported!\")\n",
        "    print(f\"\\nVersions:\")\n",
        "    print(f\"  ‚Ä¢ NumPy: {np.__version__}\")\n",
        "    print(f\"  ‚Ä¢ Pandas: {pd.__version__}\")\n",
        "    print(f\"  ‚Ä¢ Scikit-learn: imported\")\n",
        "    print(f\"  ‚Ä¢ Scipy: imported\")\n",
        "    print(f\"  ‚Ä¢ GEOparse: imported\")\n",
        "    print(f\"\\n‚úì Directories created: results/, data/\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ ENVIRONMENT READY! Proceed to Step 2\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n‚ùå ERROR: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Did you restart the kernel? (If not, do it now!)\")\n",
        "    print(\"2. Wait 30 seconds after restarting\")\n",
        "    print(\"3. Run this cell again\")\n",
        "    print(\"\\nIf still failing, try:\")\n",
        "    print(\"  Google Colab: Runtime ‚Üí Restart Session\")\n",
        "    print(\"  Jupyter: Kernel ‚Üí Restart Kernel\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gi4W8lue3AW"
      },
      "source": [
        "##  STEP 2: DEFINE DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3lBHa2Ce3AW",
        "outputId": "c7b638f4-67da-464d-9498-d873dff56659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Defining 13 GEO Datasets\n",
            "================================================================================\n",
            "Selected 12 datasets\n",
            "  ‚úì GSE10072\n",
            "  ‚úì GSE19188\n",
            "  ‚úì GSE32863\n",
            "  ‚úì GSE40791\n",
            "  ‚úì GSE75037\n",
            "  ... and 7 more\n",
            "\n",
            "‚è≥ Downloads will begin in next step...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 2: Defining 13 GEO Datasets\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "geo_accessions = [\n",
        "    'GSE10072',   # 58T + 49N\n",
        "    'GSE19188',   # 91T + 65N\n",
        "    'GSE32863',   # 58T + 58N (BALANCED)\n",
        "    'GSE40791',   # 100T + 100N (LARGEST)\n",
        "    'GSE75037',   # 83T + 83N (RECENT)\n",
        "    'GSE32665',   # Additional\n",
        "    'GSE31210',   # Additional\n",
        "    'GSE43767',   # Additional\n",
        "    'GSE30219',   # Additional\n",
        "    'GSE19804',   # Additional\n",
        "    'GSE102287',  # Additional\n",
        "    'GSE40419',   # Additional\n",
        "]\n",
        "\n",
        "print(f\"Selected {len(geo_accessions)} datasets\")\n",
        "for acc in geo_accessions[:5]:\n",
        "    print(f\"  ‚úì {acc}\")\n",
        "print(f\"  ... and {len(geo_accessions)-5} more\")\n",
        "print(f\"\\n‚è≥ Downloads will begin in next step...\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqivGXQke3AW"
      },
      "source": [
        "##  STEP 3: DOWNLOAD DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deL8LaIwe3AX",
        "outputId": "e82b60a3-0d91-421e-d1c1-55c570646d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Downloading Datasets\n",
            "================================================================================\n",
            "This may take 10-30 minutes depending on internet speed...\n",
            "\n",
            "  Downloading GSE10072... ‚úì (107 samples)\n",
            "  Downloading GSE19188... ‚úì (156 samples)\n",
            "  Downloading GSE32863... ‚úì (116 samples)\n",
            "  Downloading GSE40791... ‚úì (194 samples)\n",
            "  Downloading GSE75037... ‚úì (166 samples)\n",
            "  Downloading GSE32665... ‚úì (179 samples)\n",
            "  Downloading GSE31210... ‚úì (246 samples)\n",
            "  Downloading GSE43767... ‚úì (113 samples)\n",
            "  Downloading GSE30219... ‚úì (307 samples)\n",
            "  Downloading GSE19804... ‚úì (120 samples)\n",
            "  Downloading GSE102287... ‚úì (245 samples)\n",
            "  Downloading GSE40419... ‚úì (164 samples)\n",
            "\n",
            "‚úÖ Downloaded 12/12 datasets\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 3: Downloading Datasets\")\n",
        "print(\"=\"*80)\n",
        "print(\"This may take 10-30 minutes depending on internet speed...\\n\")\n",
        "\n",
        "def download_geo_dataset(accession, max_retries=2):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Downloading {accession}...\", end=\" \")\n",
        "            gse = GEOparse.get_GEO(geo=accession, destdir=\"./data/\", silent=True)\n",
        "            print(f\"‚úì ({len(gse.gsms)} samples)\")\n",
        "            return gse\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Failed, retrying...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print(f\"Skipped\")\n",
        "    return None\n",
        "\n",
        "datasets = {}\n",
        "for acc in geo_accessions:\n",
        "    gse = download_geo_dataset(acc)\n",
        "    if gse is not None:\n",
        "        datasets[acc] = gse\n",
        "\n",
        "print(f\"\\n‚úÖ Downloaded {len(datasets)}/{len(geo_accessions)} datasets\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eeVudA6e3AX"
      },
      "source": [
        "##  STEP 4: EXTRACT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ0amSGGe3AX",
        "outputId": "5dc3680b-c88d-4d43-a41b-51afb6e5e54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Extracting Expression & Phenotype Data\n",
            "================================================================================\n",
            "  ‚úì GSE10072: 22283 genes √ó 107 samples\n",
            "  ‚úì GSE19188: 54675 genes √ó 156 samples\n",
            "  ‚úì GSE32863: 48803 genes √ó 116 samples\n",
            "  ‚úì GSE40791: 54675 genes √ó 194 samples\n",
            "  ‚úì GSE75037: 48803 genes √ó 166 samples\n",
            "  ‚úì GSE32665: 48701 genes √ó 179 samples\n",
            "  ‚úì GSE31210: 54675 genes √ó 246 samples\n",
            "  ‚úì GSE43767: 41093 genes √ó 113 samples\n",
            "  ‚úì GSE30219: 54675 genes √ó 307 samples\n",
            "  ‚úì GSE19804: 54675 genes √ó 120 samples\n",
            "  ‚úì GSE102287: 54675 genes √ó 245 samples\n",
            "\n",
            "‚úÖ Extracted 11 datasets\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 4: Extracting Expression & Phenotype Data\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def extract_expression_matrix(gse):\n",
        "    expression_data = []\n",
        "    sample_names = []\n",
        "    for gsm_name, gsm in gse.gsms.items():\n",
        "        sample_names.append(gsm_name)\n",
        "        if 'VALUE' in gsm.table.columns:\n",
        "            expression_data.append(gsm.table['VALUE'].values)\n",
        "        else:\n",
        "            expression_data.append(gsm.table.iloc[:, 1].values)\n",
        "    first_sample = list(gse.gsms.values())[0]\n",
        "    gene_ids = first_sample.table['ID_REF'].values if 'ID_REF' in first_sample.table.columns else first_sample.table.iloc[:, 0].values\n",
        "    expr_df = pd.DataFrame(expression_data, index=sample_names, columns=gene_ids).T\n",
        "    return expr_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "def extract_phenotype_data(gse):\n",
        "    phenotype_data = []\n",
        "    tumor_keys = ['tumor', 'cancer', 'carcinoma']\n",
        "    normal_keys = ['normal', 'adjacent', 'control']\n",
        "    for gsm_name, gsm in gse.gsms.items():\n",
        "        text = str(gsm.metadata.get('characteristics_ch1', [])).lower() + str(gsm.metadata.get('title', '')).lower()\n",
        "        is_tumor = any(k in text for k in tumor_keys)\n",
        "        is_normal = any(k in text for k in normal_keys)\n",
        "        if is_tumor and not is_normal:\n",
        "            group = 'tumor'\n",
        "        elif is_normal:\n",
        "            group = 'normal'\n",
        "        else:\n",
        "            continue\n",
        "        phenotype_data.append({'sample': gsm_name, 'group': group})\n",
        "    return pd.DataFrame(phenotype_data)\n",
        "\n",
        "expression_matrices = {}\n",
        "phenotype_data_all = {}\n",
        "for acc, gse in datasets.items():\n",
        "    try:\n",
        "        expr = extract_expression_matrix(gse)\n",
        "        pheno = extract_phenotype_data(gse)\n",
        "        if len(pheno) > 0:\n",
        "            expression_matrices[acc] = expr\n",
        "            phenotype_data_all[acc] = pheno\n",
        "            print(f\"  ‚úì {acc}: {expr.shape[0]} genes √ó {expr.shape[1]} samples\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"\\n‚úÖ Extracted {len(expression_matrices)} datasets\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCVHisZ2e3AX"
      },
      "source": [
        "##  STEP 5: MERGE & QC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z8qjqwGe3AX",
        "outputId": "0ea6972f-98b8-4c88-8a30-8f930579e584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5: Quality Control & Merging (with Gene Symbol Mapping)\n",
            "================================================================================\n",
            "  Processing GSE10072 for gene mapping...\n",
            "    Downloading GPL GPL96 for mapping... ‚úì\n",
            "    ‚úì GSE10072 mapped to 14295 gene symbols.\n",
            "  Processing GSE19188 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE19188 mapped to 31773 gene symbols.\n",
            "  Processing GSE32863 for gene mapping...\n",
            "    Downloading GPL GPL6884 for mapping... ‚úì\n",
            "    No common gene symbol column found in GPL GPL6884. Cannot map.\n",
            "    Skipping gene mapping for GSE32863 due to missing GPL info or mapping issues.\n",
            "  Processing GSE40791 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE40791 mapped to 31773 gene symbols.\n",
            "  Processing GSE75037 for gene mapping...\n",
            "    Downloading GPL GPL6884 for mapping... ‚úì\n",
            "    No common gene symbol column found in GPL GPL6884. Cannot map.\n",
            "    Skipping gene mapping for GSE75037 due to missing GPL info or mapping issues.\n",
            "  Processing GSE32665 for gene mapping...\n",
            "    Downloading GPL GPL6102 for mapping... ‚úì\n",
            "    No common gene symbol column found in GPL GPL6102. Cannot map.\n",
            "    Skipping gene mapping for GSE32665 due to missing GPL info or mapping issues.\n",
            "  Processing GSE31210 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE31210 mapped to 31773 gene symbols.\n",
            "  Processing GSE43767 for gene mapping...\n",
            "    Downloading GPL GPL6480 for mapping... ‚úì\n",
            "    ‚úì GSE43767 mapped to 29752 gene symbols.\n",
            "  Processing GSE30219 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE30219 mapped to 31773 gene symbols.\n",
            "  Processing GSE19804 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE19804 mapped to 31773 gene symbols.\n",
            "  Processing GSE102287 for gene mapping...\n",
            "    Downloading GPL GPL570 for mapping... ‚úì\n",
            "    ‚úì GSE102287 mapped to 31773 gene symbols.\n",
            "  Genes: 16\n",
            "  Samples: 1488\n",
            "  Tumor: 996\n",
            "  Normal: 383\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 5: Quality Control & Merging (with Gene Symbol Mapping)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Dictionary to store mapped expression matrices\n",
        "mapped_expression_matrices = {}\n",
        "\n",
        "# Function to get gene symbols from a GPL entry\n",
        "# This is a simplified function and might need refinement for various GPL formats\n",
        "def get_gene_symbols_from_gpl(gpl_id, gpl_data):\n",
        "    # Try to find a column that looks like gene symbol or similar\n",
        "    # Common names: 'Gene Symbol', 'Gene_Symbol', 'GENE_SYMBOL', 'gene_assignment', 'Associated Gene Name', 'gene_name']\n",
        "    gene_symbol_cols = ['Gene Symbol', 'Gene_Symbol', 'GENE_SYMBOL', 'gene_assignment', 'Associated Gene Name', 'gene_name']\n",
        "\n",
        "    # Check if GPL data is already loaded, otherwise load it\n",
        "    # Fix: Check the class name of the gpl_data object instead of GEOparse.GEOTable\n",
        "    if gpl_data.__class__.__name__ == 'GEOTable':\n",
        "        gpl_df = gpl_data.table\n",
        "    else: # Assume gpl_id is passed and need to download\n",
        "        try:\n",
        "            print(f\"    Downloading GPL {gpl_id} for mapping...\", end=\" \")\n",
        "            gpl = GEOparse.get_GEO(geo=gpl_id, destdir=\"./data/\", silent=True)\n",
        "            gpl_df = gpl.table\n",
        "            print(\"‚úì\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download GPL {gpl_id}: {e}. Skipping mapping for this dataset.\")\n",
        "            return None, None # Return None if mapping failed\n",
        "\n",
        "    # Find the ID_REF column (probe IDs)\n",
        "    id_ref_col = None\n",
        "    if 'ID' in gpl_df.columns:\n",
        "        id_ref_col = 'ID'\n",
        "    elif 'ID_REF' in gpl_df.columns:\n",
        "        id_ref_col = 'ID_REF'\n",
        "\n",
        "    if not id_ref_col:\n",
        "        print(f\"    No ID_REF column found in GPL {gpl_id}. Cannot map.\")\n",
        "        return None, None\n",
        "\n",
        "    # Find gene symbol column\n",
        "    gene_col = None\n",
        "    for col in gene_symbol_cols:\n",
        "        if col in gpl_df.columns:\n",
        "            gene_col = col\n",
        "            break\n",
        "\n",
        "    if not gene_col:\n",
        "        print(f\"    No common gene symbol column found in GPL {gpl_id}. Cannot map.\")\n",
        "        return None, None\n",
        "\n",
        "    # Create mapping dictionary from probe ID to gene symbol\n",
        "    # Handle cases where multiple probes map to one gene symbol, or one probe to multiple symbols\n",
        "    # Simplification: take the first gene symbol if multiple are present (e.g., delimited by ///)\n",
        "    mapping = {}\n",
        "    for idx, row in gpl_df.iterrows():\n",
        "        probe_id = row[id_ref_col]\n",
        "        gene_symbols_raw = str(row[gene_col]).split(' /// ')[0].strip() # Take first if multiple\n",
        "        if gene_symbols_raw and gene_symbols_raw != 'nan':\n",
        "            mapping[probe_id] = gene_symbols_raw\n",
        "\n",
        "    return mapping, gpl_df\n",
        "\n",
        "\n",
        "# First, extract platform information for each dataset\n",
        "platform_info = {}\n",
        "for acc, gse in datasets.items():\n",
        "    if gse.gpls: # Check if GPLs exist\n",
        "        platform_info[acc] = list(gse.gpls.keys())[0] # Take the first GPL if multiple\n",
        "    else:\n",
        "        print(f\"  WARNING: No platform (GPL) info found for {acc}. Skipping gene mapping for this dataset.\")\n",
        "        platform_info[acc] = None\n",
        "\n",
        "# Perform gene symbol mapping for each dataset\n",
        "for acc, expr_df in expression_matrices.items():\n",
        "    gpl_id = platform_info.get(acc)\n",
        "    if gpl_id:\n",
        "        print(f\"  Processing {acc} for gene mapping...\")\n",
        "        gpl_table_data = datasets[acc].gpls[gpl_id] # Pass the already downloaded GPL table if available\n",
        "        probe_to_gene_map, _ = get_gene_symbols_from_gpl(gpl_id, gpl_table_data)\n",
        "\n",
        "        if probe_to_gene_map:\n",
        "            # Create a new index with gene symbols\n",
        "            mapped_index = [probe_to_gene_map.get(probe, probe) for probe in expr_df.index]\n",
        "            mapped_expr_df = expr_df.copy()\n",
        "            mapped_expr_df.index = mapped_index\n",
        "\n",
        "            # Handle duplicate gene symbols by averaging their expression\n",
        "            mapped_expr_df = mapped_expr_df.groupby(mapped_expr_df.index).mean()\n",
        "\n",
        "            mapped_expression_matrices[acc] = mapped_expr_df\n",
        "            print(f\"    ‚úì {acc} mapped to {len(mapped_expr_df.index)} gene symbols.\")\n",
        "        else:\n",
        "            print(f\"    Skipping gene mapping for {acc} due to missing GPL info or mapping issues.\")\n",
        "            # If mapping fails, fall back to original probes, but this will likely fail merging\n",
        "            # or simply exclude this dataset from the merged_expr. Let's exclude it for now.\n",
        "    else:\n",
        "        print(f\"  Skipping gene mapping for {acc} (no GPL info).\")\n",
        "\n",
        "\n",
        "# Now, perform QC on mapped expression matrices and merge\n",
        "qc_expr = {}\n",
        "for acc, expr in mapped_expression_matrices.items():\n",
        "    # Drop rows (genes) with too many NaNs\n",
        "    expr = expr.dropna(thresh=expr.shape[1] * 0.5) # require at least 50% non-NaN values\n",
        "    # Filter by variance (genes with top 50% variance)\n",
        "    if not expr.empty and expr.var(axis=1).count() > 0: # Ensure there are genes to filter by variance\n",
        "        expr = expr[expr.var(axis=1) > expr.var(axis=1).quantile(0.5)]\n",
        "    else:\n",
        "        print(f\"    WARNING: {acc} has too few genes after NaN removal or no variance to filter. Skipping variance filter.\")\n",
        "    qc_expr[acc] = expr\n",
        "\n",
        "all_expr_filtered = list(qc_expr.values())\n",
        "all_pheno_filtered = [phenotype_data_all[acc] for acc in qc_expr.keys()]\n",
        "\n",
        "if not all_expr_filtered:\n",
        "    print(\"  ERROR: No expression data remaining after gene mapping and QC. Cannot merge.\")\n",
        "    merged_expr = pd.DataFrame()\n",
        "    merged_pheno = pd.DataFrame() # Ensure pheno is also empty or aligned\n",
        "else:\n",
        "    # Find common genes across all *mapped and QC-filtered* datasets\n",
        "    common_genes = set(all_expr_filtered[0].index)\n",
        "    for expr in all_expr_filtered[1:]:\n",
        "        common_genes = common_genes.intersection(set(expr.index))\n",
        "    common_genes = list(common_genes)\n",
        "\n",
        "    if not common_genes:\n",
        "        print(\"  WARNING: No common gene symbols found across all mapped datasets after QC. Merged expression matrix will be empty.\")\n",
        "        merged_expr = pd.DataFrame()\n",
        "        merged_pheno = pd.DataFrame() # Ensure pheno is also empty or aligned\n",
        "    else:\n",
        "        all_expr_common_genes = [expr.loc[common_genes] for expr in all_expr_filtered]\n",
        "        merged_expr = pd.concat(all_expr_common_genes, axis=1)\n",
        "        merged_pheno = pd.concat(all_pheno_filtered, ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"  Genes: {merged_expr.shape[0]}\")\n",
        "print(f\"  Samples: {merged_expr.shape[1]}\")\n",
        "print(f\"  Tumor: {(merged_pheno['group']=='tumor').sum()}\")\n",
        "print(f\"  Normal: {(merged_pheno['group']=='normal').sum()}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZrbNuT6e3AX"
      },
      "source": [
        "##  STEP 6: DIFFERENTIAL EXPRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCgTKD3Ge3AX",
        "outputId": "9a4a50c2-5f62-48ee-936d-47428ba68c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Differential Expression Analysis\n",
            "================================================================================\n",
            "  DEGs: 10\n",
            "  Upregulated: 9\n",
            "  Downregulated: 1\n",
            "  ‚úì Saved: deg_results.csv\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 6: Differential Expression Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if merged_expr.shape[0] == 0:\n",
        "    print(\"  WARNING: No common genes were found across the datasets after QC (merged_expr has 0 rows).\")\n",
        "    print(\"  Differential expression analysis cannot be performed.\")\n",
        "    # Initialize deg_df as an empty DataFrame with expected columns to prevent downstream errors\n",
        "    deg_df = pd.DataFrame(columns=['gene', 't_stat', 'p_value', 'log2_fc', 'adj_p', 'status'])\n",
        "    deg_genes = []\n",
        "else:\n",
        "    tumor_expr = merged_expr[merged_pheno[merged_pheno['group']=='tumor']['sample']]\n",
        "    normal_expr = merged_expr[merged_pheno[merged_pheno['group']=='normal']['sample']]\n",
        "\n",
        "    results = []\n",
        "    for gene in merged_expr.index:\n",
        "        # Check for non-empty series for t-test and sufficient samples\n",
        "        tumor_values = tumor_expr.loc[gene].dropna()\n",
        "        normal_values = normal_expr.loc[gene].dropna()\n",
        "\n",
        "        if len(tumor_values) >= 2 and len(normal_values) >= 2: # t-test needs at least 2 samples per group\n",
        "            try:\n",
        "                # Use Welch's t-test by setting equal_var=False\n",
        "                t_stat, p_val = stats.ttest_ind(tumor_values, normal_values, equal_var=False, nan_policy='omit')\n",
        "\n",
        "                # Calculate fold change, handling potential division by zero\n",
        "                normal_mean = normal_values.mean()\n",
        "                if normal_mean != 0:\n",
        "                    fc = np.log2(tumor_values.mean() / (normal_mean + 1e-6))\n",
        "                else:\n",
        "                    fc = np.nan # Assign NaN if normal mean is effectively zero\n",
        "\n",
        "                results.append({'gene': gene, 't_stat': t_stat, 'p_value': p_val, 'log2_fc': fc})\n",
        "            except Exception as e:\n",
        "                # Catch any unexpected errors during t-test calculation for a gene\n",
        "                results.append({'gene': gene, 't_stat': np.nan, 'p_value': np.nan, 'log2_fc': np.nan})\n",
        "        else:\n",
        "            # Not enough samples to perform t-test for this gene\n",
        "            results.append({'gene': gene, 't_stat': np.nan, 'p_value': np.nan, 'log2_fc': np.nan})\n",
        "\n",
        "    deg_df = pd.DataFrame(results)\n",
        "\n",
        "    # Only perform multiple testing correction if deg_df is not empty and has 'p_value' column\n",
        "    if not deg_df.empty and 'p_value' in deg_df.columns and not deg_df['p_value'].isnull().all():\n",
        "        # Filter out NaN p_values before applying multipletests\n",
        "        valid_p_values_idx = deg_df['p_value'].dropna().index\n",
        "        if not valid_p_values_idx.empty:\n",
        "            rejected, adj_p, _, _ = multipletests(deg_df.loc[valid_p_values_idx, 'p_value'], method='fdr_bh')\n",
        "            deg_df['adj_p'] = np.nan # Initialize with NaN\n",
        "            deg_df.loc[valid_p_values_idx, 'adj_p'] = adj_p\n",
        "            deg_df['status'] = 'not_sig' # Default status\n",
        "            deg_df.loc[(deg_df['log2_fc'] > 1) & (deg_df['adj_p'] < 0.05), 'status'] = 'upregulated'\n",
        "            deg_df.loc[(deg_df['log2_fc'] < -1) & (deg_df['adj_p'] < 0.05), 'status'] = 'downregulated'\n",
        "        else:\n",
        "            # No valid p-values for correction\n",
        "            deg_df['adj_p'] = np.nan\n",
        "            deg_df['status'] = 'not_sig'\n",
        "    else:\n",
        "        # deg_df is empty, no 'p_value' column, or all 'p_value' are NaN\n",
        "        deg_df['adj_p'] = np.nan\n",
        "        deg_df['status'] = 'not_sig'\n",
        "\n",
        "    deg_genes = deg_df[deg_df['status'] != 'not_sig']['gene'].tolist()\n",
        "\n",
        "print(f\"  DEGs: {len(deg_genes)}\")\n",
        "print(f\"  Upregulated: {(deg_df['status']=='upregulated').sum() if not deg_df.empty and 'status' in deg_df.columns else 0}\")\n",
        "print(f\"  Downregulated: {(deg_df['status']=='downregulated').sum() if not deg_df.empty and 'status' in deg_df.columns else 0}\")\n",
        "\n",
        "if not deg_df.empty:\n",
        "    deg_df.to_csv('results/deg_results.csv', index=False)\n",
        "    print(\"  ‚úì Saved: deg_results.csv\")\n",
        "else:\n",
        "    print(\"  No differential expression results to save as deg_df is empty.\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr0dd_T2e3AY"
      },
      "source": [
        "##  STEP 7: GENE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGRxQ_cye3AY",
        "outputId": "8b51e1ba-8459-4f14-821b-1f123af11cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7: Core Gene Selection\n",
            "================================================================================\n",
            "  Selected: 16 genes\n",
            "    1. DSG2 (FC: 3.37)\n",
            "    2. IRF6 (FC: 2.53)\n",
            "    3. GAMT (FC: 1.25)\n",
            "    4. CEP170 (FC: 1.93)\n",
            "    5. PTPRC (FC: 1.73)\n",
            "    ... and 11 more\n",
            "  ‚úì Saved: core_genes.csv\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 7: Core Gene Selection\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "core_genes = deg_df.nlargest(20, 't_stat')['gene'].tolist() if len(deg_genes) > 0 else deg_df.nlargest(15, 't_stat')['gene'].tolist()\n",
        "\n",
        "print(f\"  Selected: {len(core_genes)} genes\")\n",
        "for i, gene in enumerate(core_genes[:5], 1):\n",
        "    fc = deg_df[deg_df['gene']==gene]['log2_fc'].values[0]\n",
        "    print(f\"    {i}. {gene} (FC: {fc:.2f})\")\n",
        "if len(core_genes) > 5:\n",
        "    print(f\"    ... and {len(core_genes)-5} more\")\n",
        "\n",
        "pd.DataFrame({'gene': core_genes, 'rank': range(1, len(core_genes)+1)}).to_csv('results/core_genes.csv', index=False)\n",
        "print(\"  ‚úì Saved: core_genes.csv\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHbLI_GXe3AY"
      },
      "source": [
        "##  STEP 8: MACHINE LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4xmvPmwe3AY",
        "outputId": "18c8ebb7-e935-4ddd-f260-7954e74c306c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8: Machine Learning Classification\n",
            "================================================================================\n",
            "  Samples: 1379\n",
            "  Features: 16\n",
            "  Classes: T=996, N=383\n",
            "  RF: AUC=0.987, Acc=0.952\n",
            "  GB: AUC=0.981, Acc=0.949\n",
            "  LR: AUC=0.866, Acc=0.800\n",
            "  SVM: AUC=0.928, Acc=0.800\n",
            "\n",
            "  üèÜ Best: RF (AUC=0.987)\n",
            "  ‚úì Saved: model_comparison.csv\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 8: Machine Learning Classification\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ensure merged_expr and merged_pheno have matching samples in the same order\n",
        "# Get samples that are common to both\n",
        "common_samples = list(set(merged_expr.columns).intersection(set(merged_pheno['sample'])))\n",
        "\n",
        "# Filter merged_expr to only include common samples\n",
        "merged_expr_aligned = merged_expr[common_samples]\n",
        "\n",
        "# Filter merged_pheno to only include common samples and ensure order matches merged_expr_aligned\n",
        "merged_pheno_aligned = merged_pheno[merged_pheno['sample'].isin(common_samples)]\n",
        "merged_pheno_aligned = merged_pheno_aligned.set_index('sample').loc[merged_expr_aligned.columns].reset_index()\n",
        "\n",
        "# Now create X and y from the aligned data\n",
        "X = merged_expr_aligned.loc[core_genes].T.values\n",
        "y = (merged_pheno_aligned['group']=='tumor').astype(int).values\n",
        "\n",
        "print(f\"  Samples: {X.shape[0]}\")\n",
        "print(f\"  Features: {X.shape[1]}\")\n",
        "print(f\"  Classes: T={y.sum()}, N={len(y)-y.sum()}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'GB': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'LR': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "results_ml = []\n",
        "for name, model in models.items():\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    test_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results_ml.append({'Model': name, 'CV_AUC': cv_scores.mean(), 'Test_AUC': test_auc, 'Acc': acc})\n",
        "    print(f\"  {name}: AUC={test_auc:.3f}, Acc={acc:.3f}\")\n",
        "\n",
        "results_ml_df = pd.DataFrame(results_ml).sort_values('Test_AUC', ascending=False)\n",
        "best_model_name = results_ml_df.iloc[0]['Model']\n",
        "best_auc = results_ml_df.iloc[0]['Test_AUC']\n",
        "\n",
        "print(f\"\\n  üèÜ Best: {best_model_name} (AUC={best_auc:.3f})\")\n",
        "results_ml_df.to_csv('results/model_comparison.csv', index=False)\n",
        "print(\"  ‚úì Saved: model_comparison.csv\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vNrbKfJe3AY"
      },
      "source": [
        "##  STEP 9: VISUALIZATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCfV3OAFe3AY",
        "outputId": "29411cc3-2950-4059-ad75-bbd304192957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9: Creating Visualizations\n",
            "================================================================================\n",
            "  ‚úì ROC curve saved\n",
            "  ‚úì Confusion matrix saved\n",
            "  ‚úì Volcano plot saved\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStep 9: Creating Visualizations\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train final model for plotting\n",
        "best_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# ROC Curve\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='#e74c3c', lw=3, label=f'ROC (AUC={roc_auc:.3f})')\n",
        "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
        "plt.xlabel('FPR', fontsize=12)\n",
        "plt.ylabel('TPR', fontsize=12)\n",
        "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/roc_curve.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"  ‚úì ROC curve saved\")\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = best_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"  ‚úì Confusion matrix saved\")\n",
        "\n",
        "# Volcano Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "for status in deg_df['status'].unique():\n",
        "    if status == 'not_sig':\n",
        "        subset = deg_df[deg_df['status']==status]\n",
        "        plt.scatter(subset['log2_fc'], -np.log10(subset['adj_p']), alpha=0.3, s=20, label='not sig')\n",
        "    elif status == 'upregulated':\n",
        "        subset = deg_df[deg_df['status']==status]\n",
        "        plt.scatter(subset['log2_fc'], -np.log10(subset['adj_p']), color='red', s=50, label='up')\n",
        "    else:\n",
        "        subset = deg_df[deg_df['status']==status]\n",
        "        plt.scatter(subset['log2_fc'], -np.log10(subset['adj_p']), color='blue', s=50, label='down')\n",
        "\n",
        "plt.axvline(1, ls='--', alpha=0.7)\n",
        "plt.axvline(-1, ls='--', alpha=0.7)\n",
        "plt.xlabel('Log2 FC')\n",
        "plt.ylabel('-Log10 P')\n",
        "plt.title('Volcano Plot', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/volcano_plot.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"  ‚úì Volcano plot saved\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_w5XDcke3AY"
      },
      "source": [
        "##  STEP 10: FINAL SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVjy-69Fe3AZ",
        "outputId": "89fce4ca-ee4e-40ad-ef7e-ddd32389eb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE - FINAL SUMMARY\n",
            "================================================================================\n",
            "\n",
            "DATASETS: 11\n",
            "SAMPLES: 1488 total\n",
            "  Tumor: 996\n",
            "  Normal: 383\n",
            "\n",
            "GENES: 16 (QC filtered)\n",
            "DEGS: 10 significant\n",
            "  Up: 9\n",
            "  Down: 1\n",
            "\n",
            "CORE GENES: 16\n",
            "BEST MODEL: RF\n",
            "  AUC: 0.987\n",
            "\n",
            "OUTPUT FILES:\n",
            "  ‚úì deg_results.csv\n",
            "  ‚úì core_genes.csv\n",
            "  ‚úì model_comparison.csv\n",
            "  ‚úì roc_curve.png\n",
            "  ‚úì confusion_matrix.png\n",
            "  ‚úì volcano_plot.png\n",
            "\n",
            "‚úì Summary saved\n",
            "\n",
            "üéâ ALL DONE! Check results/ folder!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE - FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = f\"\"\"\n",
        "DATASETS: {len(expression_matrices)}\n",
        "SAMPLES: {merged_expr.shape[1]} total\n",
        "  Tumor: {(merged_pheno['group']=='tumor').sum()}\n",
        "  Normal: {(merged_pheno['group']=='normal').sum()}\n",
        "\n",
        "GENES: {merged_expr.shape[0]} (QC filtered)\n",
        "DEGS: {len(deg_genes)} significant\n",
        "  Up: {(deg_df['status']=='upregulated').sum()}\n",
        "  Down: {(deg_df['status']=='downregulated').sum()}\n",
        "\n",
        "CORE GENES: {len(core_genes)}\n",
        "BEST MODEL: {best_model_name}\n",
        "  AUC: {best_auc:.3f}\n",
        "\n",
        "OUTPUT FILES:\n",
        "  ‚úì deg_results.csv\n",
        "  ‚úì core_genes.csv\n",
        "  ‚úì model_comparison.csv\n",
        "  ‚úì roc_curve.png\n",
        "  ‚úì confusion_matrix.png\n",
        "  ‚úì volcano_plot.png\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "with open('results/SUMMARY.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "print(\"‚úì Summary saved\")\n",
        "print(\"\\nüéâ ALL DONE! Check results/ folder!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}